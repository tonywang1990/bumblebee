{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonywy/micromamba/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "data_dir=\"data/shakespeare_char\"\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r').astype(int)\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r').astype(int) \n",
    "meta = pickle.load(open(os.path.join(data_dir, 'meta.pkl'), 'rb'))\n",
    "stoi=meta['stoi']\n",
    "itos=meta['itos']\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Check that MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"mps\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is 65\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 16 # B\n",
    "block_size = 128 # T\n",
    "n_emb = 6*16 # E\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "#head_size = n_emb # H\n",
    "vocab_size = meta['vocab_size'] # C\n",
    "num_epoch = 5000\n",
    "learning_rate = 3e-4\n",
    "print(f\"vocab size is {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if split == 'train':\n",
    "        data = torch.from_numpy(np.array(train_data))\n",
    "    elif split == 'val':\n",
    "        data = torch.from_numpy(np.array(val_data))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    ids = torch.randint(low=0, high=len(data)-block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ids])\n",
    "    y = torch.stack([data[i+1 : i+block_size+1] for i in ids])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb=get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, emb_size: int, head_size: int):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(emb_size, head_size)\n",
    "        self.key = nn.Linear(emb_size, head_size)\n",
    "        self.value = nn.Linear(emb_size, head_size)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, emb: torch.Tensor) -> torch.Tensor:\n",
    "        # input: (B, T, E)\n",
    "        B, T, E = emb.shape\n",
    "        q = self.query(emb) # q,k,v = (B, T, H)\n",
    "        k = self.key(emb)\n",
    "        v = self.value(emb)\n",
    "        weight = q @ k.transpose(-2, -1) * (E**-0.5) # (B, T, T)\n",
    "        # As a decoder block, mask out future information\n",
    "        weight = weight.masked_fill(self.tril[:T, :T] == 0, -torch.inf)\n",
    "        weight = torch.softmax(weight, dim=-1)\n",
    "        out = weight @ v # (B, T, H)\n",
    "        return out\n",
    "\n",
    "head = Head(n_emb, 8)\n",
    "out = head(torch.ones((batch_size, block_size, n_emb)))\n",
    "assert out.shape == torch.Size([batch_size, block_size, 8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 96])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int, n_head: int, head_size: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(emb_size, head_size) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(emb_size, emb_size)\n",
    "    \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor: \n",
    "        x = torch.cat([h(input) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "mhead = MultiHeadAttention(n_emb, 4, n_emb//4)\n",
    "out = mhead(torch.ones((batch_size, block_size, n_emb)))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 96])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb,n_emb),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor: \n",
    "        return self.net(input)\n",
    "\n",
    "mhead = FeedForward(n_emb)\n",
    "out = mhead(torch.ones((batch_size, block_size, n_emb)))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 96])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, emb_size: int, n_head:int):\n",
    "        super().__init__()\n",
    "        self.sa_head = MultiHeadAttention(emb_size, n_head, n_emb//n_head)\n",
    "        self.feed_forward = FeedForward(emb_size)\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "    \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = input + self.sa_head(self.ln1(input)) # residual connection, pre-norm\n",
    "        x = x + self.feed_forward(self.ln2(x)) # residual connection, pre-norm\n",
    "        return x\n",
    "\n",
    "block = Block(n_emb, n_head)\n",
    "out = block(torch.ones((batch_size, block_size, n_emb)))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ":AwBtSy;o!AVHNMWCtBhsBT3IM$CyQvXnrRzP,t?udTHSyn,cSITSh;y;eId3&frDMcIHq-YUhtjVOXcofFFK-xjhIT!ExYU\n",
      "NWe\n"
     ]
    }
   ],
   "source": [
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_emb, n_head) for _ in range(n_layer)],\n",
    "            nn.LayerNorm(n_emb)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # idx shape = [B, T]\n",
    "        idx = idx[:, -block_size:]\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embedding_table(idx) # [B, T, E]\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # [T, E]\n",
    "        x = token_emb + pos_emb # [B, T, E]\n",
    "        x = self.blocks(x) # [B, T, H=E]\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits=logits.view(batch_size * block_size, vocab_size)\n",
    "            targets = targets.view(batch_size * block_size)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(idx[:, -block_size:]) # [B, T ,C]\n",
    "            logits = logits[:, -1, :] # [B, C]\n",
    "            probs = torch.softmax(logits, dim=-1) # [B, C]\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # [B, 1]\n",
    "            idx = torch.cat((idx, idx_next), dim=-1) # [B, C+1]\n",
    "        return idx \n",
    "\n",
    "model = BigramModel().to(device)\n",
    "output = model.generate(torch.zeros((1,1), dtype=torch.int, device=device), 100)[0].tolist()\n",
    "print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(4.2996), 'val': tensor(4.2898)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters: int = 10):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            input, targets = get_batch(split)\n",
    "            _, loss = model(input, targets)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "        \n",
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/5000 [00:01<42:19,  1.97it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(4.1408), 'val': tensor(4.1466)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 502/5000 [01:09<26:05,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(2.4675), 'val': tensor(2.4749)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1002/5000 [02:19<26:30,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(2.3021), 'val': tensor(2.3352)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1501/5000 [03:30<27:53,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(2.1683), 'val': tensor(2.1948)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2002/5000 [04:43<17:46,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(2.0289), 'val': tensor(2.0947)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2502/5000 [05:55<15:16,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(1.9374), 'val': tensor(2.0111)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3002/5000 [07:07<11:38,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(1.8349), 'val': tensor(2.0070)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3502/5000 [08:21<09:53,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(1.7746), 'val': tensor(1.9153)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4002/5000 [09:35<06:45,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(1.7340), 'val': tensor(1.8768)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4502/5000 [10:48<03:04,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(1.6653), 'val': tensor(1.8240)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [12:00<00:00,  6.94it/s]\n"
     ]
    }
   ],
   "source": [
    "def train(model: nn.Module):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for i in tqdm(range(num_epoch)):\n",
    "        input, targets = get_batch('train')\n",
    "        logits, loss = model(input, targets)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % (num_epoch/10) == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(losses)\n",
    "\n",
    "train(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conteend yours benesss like own of no lince.\n",
      "Why the wishicess,' any like word's cars war our of death be.\n",
      "\n",
      "BUCKINGHAM:\n",
      "May musterbless are once you, you have our mary?\n",
      "When spies cuntilly accel'd,\n",
      "Marke alies loss the gurliel more strups.\n",
      "Hastile her pllop-leak over all underifurites\n",
      "Mide words grace our shis dear ranges dabe\n",
      "Admeneor and yeter a getels your their: it willows,\n",
      "Madasted me happect spect woe of livess at o'ers all\n",
      "I, may stirs both great's tream; you can make;\n",
      "If day, death &norbhad o's thy beloud namply\n",
      "cortrause a my garlown oner'd? they read cails;\n",
      "Edwand the shall I charrine is my mean brow;\n",
      "This to mose on life hams, mine like right\n",
      "Than girth the gen death kill givods the breas,\n",
      "and their of underous father, more boughters,,\n",
      "Opparnoon their, are we whast this on\n",
      "The commiatis,--that youb's naguy,\n",
      "And barrs last bose pion him.\n",
      "\n",
      "ISABELLOND:\n",
      "Uneme are shalt O adle seems me. Awh, siry wity\n",
      "These fanstence us, my in that he\n",
      "The sen so crivent my that nurnes on.\n",
      "\n",
      "GLUCEI\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(torch.zeros((1,1), dtype=torch.long, device=device), 1000)[0].tolist()\n",
    "print(decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 5\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weight = torch.zeros(T,T)\n",
    "weight = weight.masked_fill(tril==0, -torch.inf)\n",
    "weight = torch.softmax(weight, dim=-1)\n",
    "weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1667, -0.2986,  0.0434, -0.9022,  1.1912,  0.6353,  0.2423, -0.5491,\n",
      "          1.9410,  0.7559, -1.2508,  0.0583, -0.8587, -0.4010,  0.9786, -0.5261,\n",
      "          1.1254,  1.1608, -0.5628,  1.7550],\n",
      "        [ 1.3411, -1.5851,  2.0341, -0.7048,  1.2602, -1.4971, -0.1538,  1.3706,\n",
      "         -1.1876, -1.6367,  1.0304, -1.5667, -0.9364, -0.2546,  0.3365,  0.7078,\n",
      "          0.8851,  1.6311, -1.0416, -0.5219]])\n",
      "tensor([[ 1.8508, -0.6405, -0.2950, -1.2504,  0.8650,  0.3033, -0.0939, -0.8936,\n",
      "          1.6226,  0.4251, -1.6028, -0.2798, -1.2065, -0.7440,  0.6502, -0.8704,\n",
      "          0.7985,  0.8343, -0.9075,  1.4347],\n",
      "        [ 1.1414, -1.3045,  1.7206, -0.5686,  1.0738, -1.2309, -0.1081,  1.1660,\n",
      "         -0.9722, -1.3475,  0.8817, -1.2890, -0.7622, -0.1924,  0.3017,  0.6121,\n",
      "          0.7603,  1.3838, -0.8502, -0.4158]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([-5.9605e-09,  1.1921e-08], grad_fn=<MeanBackward1>)\n",
      "tensor([1.0260, 1.0260], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn((2, 20))\n",
    "ln = nn.LayerNorm(20)\n",
    "print(t)\n",
    "print(ln(t))\n",
    "print(torch.mean(ln(t), dim=1))\n",
    "print(torch.std(ln(t), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b007964239c9846de49217bea874a76b6e18c6041f326c6a02623c321aae0990"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
